{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "from openai import AzureOpenAI\n",
    "from threading import Semaphore\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from typing import List, Dict, Any\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_fixed,\n",
    "    wait_random,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Format Example\n",
    "\n",
    "The input data, `message_sequences`, should be structured as a list of message dictionaries, where each message has a `role` (either \"system\" or \"user\") and `content`. Hereâ€™s a very simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'role': 'system',\n",
       "   'content': \"You are a helpful AI assistant.  Always answer in the following json format: {'content': '2'}.\"},\n",
       "  {'role': 'user', 'content': 'What is 1 + 1?'}],\n",
       " [{'role': 'system',\n",
       "   'content': \"You are a helpful AI assistant.  Always answer in the following json format: {'content': '2'}.\"},\n",
       "  {'role': 'user', 'content': 'What is 2 + 2?'}],\n",
       " [{'role': 'system',\n",
       "   'content': \"You are a helpful AI assistant.  Always answer in the following json format: {'content': '2'}.\"},\n",
       "  {'role': 'user', 'content': 'What is 3 + 3?'}]]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message_sequences = []\n",
    "for i in range(1, 60):\n",
    "    message_sequences.append([\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.  Always answer in the following json format: {'content': '2'}.\"},\n",
    "     {\"role\": \"user\", \"content\": f\"What is {i} + {i}?\"}\n",
    "    ])\n",
    "message_sequences[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class APIPlayer:\n",
    "    def __init__(self, model_name, temperature=1.0, max_tokens=15, rate_limit=20):\n",
    "        self.client = AzureOpenAI(\n",
    "                # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning\n",
    "                api_version=\"2023-07-01-preview\",\n",
    "                # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource\n",
    "               azure_endpoint= \"https://chm-openai.openai.azure.com/\",\n",
    "            )\n",
    "        self.model_name = model_name\n",
    "        self.temperature = temperature\n",
    "        self.max_tokens = max_tokens\n",
    "        self.rate_limit = rate_limit  # max requests per minute, adjust according to your limit\n",
    "        self.semaphore = Semaphore(self.rate_limit)\n",
    "    def handle_retry_error(retry_state):\n",
    "        print(f\"All retry attempts failed for: {retry_state.args[0]}\\nReturning None for this request.\")\n",
    "        return '{\"content\": \"None\"}'  # Custom behavior after all retries fail\n",
    "\n",
    "    @retry(wait=wait_fixed(2) + wait_random(0, 2),\n",
    "            stop=stop_after_attempt(2),\n",
    "            before_sleep= lambda retry_state: print(\"Retrying...\"),\n",
    "            retry_error_callback=handle_retry_error)\n",
    "    def get_response(self, system_user_message: List):\n",
    "            with self.semaphore:\n",
    "                try:\n",
    "                    response = self.client.chat.completions.create(\n",
    "                                model=self.model_name,\n",
    "                                messages= system_user_message,\n",
    "                                temperature= self.temperature,\n",
    "                                max_tokens=self.max_tokens,\n",
    "                                response_format= { \"type\": \"json_object\" },\n",
    "                            )\n",
    "                    json_response = response.choices[0].message.content\n",
    "                    json.loads(json_response)  # Attempt to parse the JSON, will raise an error if not valid JSON\n",
    "                    time.sleep(60 / self.rate_limit)  # Pause to respect the rate limit to evenly distribute requests over time\n",
    "                    return json_response\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Invalid JSON response for message {system_user_message}\")\n",
    "                    raise  # This re-raises the last exception, triggering a retry\n",
    "                except Exception as e:\n",
    "                    print(f\"Error while processing: {str(e)}\")\n",
    "                    raise\n",
    "         \n",
    "    def get_responses_parallel(self, messages_list):\n",
    "        results = []\n",
    "        started = time.time()\n",
    "        with ThreadPoolExecutor(max_workers=self.rate_limit) as executor:\n",
    "            future_to_message = {executor.submit(self.get_response, message): message for message in messages_list}\n",
    "            for future in as_completed(future_to_message):\n",
    "                message = future_to_message[future]\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    # Extract the user's query from the message (response format is a JSON string like '{\"content\": \"4\"}')\n",
    "                    user_query = next((msg['content'] for msg in message if msg['role'] == 'user'), \"Unknown query\")\n",
    "                    result_content = json.loads(result)['content'] # if result is \n",
    "                    results.append({\"input\": user_query, \"content\": result_content})\n",
    "                except Exception as e:\n",
    "                      results.append({\"input\": \"Error\", \"content\": f\"Error processing message: {str(e)}\"})\n",
    "        print(f\"Total time taken: {time.time() - started} seconds\")\n",
    "        return results               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set `AZURE_OPENAI_API_KEY` in your environment variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"AZURE_OPENAI_API_KEY\"] = \"your_token\"\n",
    "\n",
    "## or in .env file like this:\n",
    "#AZURE_OPENAI_API_KEY = your_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure you stay within the rate limit for the selected model\n",
    "\n",
    "In Our Azure AI settings, gpt-35-turbo has a Rate limit (Requests per minute) of 720"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "player = APIPlayer(model_name = \"gpt-35-turbo\", temperature = 1.0, max_tokens = 20, rate_limit = 100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken: 1.7316970825195312 seconds\n"
     ]
    }
   ],
   "source": [
    "results_parallel = player.get_responses_parallel(message_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input': 'What is 9 + 9?', 'content': '18'},\n",
       " {'input': 'What is 4 + 4?', 'content': '8'},\n",
       " {'input': 'What is 11 + 11?', 'content': '22'},\n",
       " {'input': 'What is 5 + 5?', 'content': '10'},\n",
       " {'input': 'What is 6 + 6?', 'content': '12'}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_parallel[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As u can notice, the order of the responses is not the same as the order of the queries because the responses are processed in parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now a one-per-one request method to compare it with the parallel method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class APIPlayer_simpler:\n",
    "    def __init__(self, model_name, temperature=1.0, max_tokens=15):\n",
    "        self.client = AzureOpenAI(\n",
    "                # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning\n",
    "                api_version=\"2023-07-01-preview\",\n",
    "                # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource\n",
    "               azure_endpoint= \"https://chm-openai.openai.azure.com/\",\n",
    "            )\n",
    "        self.model_name = model_name\n",
    "        self.temperature = temperature\n",
    "        self.max_tokens = max_tokens\n",
    "\n",
    "    def get_response(self, system_user_message: List):\n",
    "                try:\n",
    "                    response = self.client.chat.completions.create(\n",
    "                                model=self.model_name,\n",
    "                                messages= system_user_message,\n",
    "                                temperature= self.temperature,\n",
    "                                max_tokens=self.max_tokens,\n",
    "                                response_format= { \"type\": \"json_object\" },\n",
    "                            )\n",
    "                    json_response = response.choices[0].message.content\n",
    "                    json.loads(json_response)  # Attempt to parse the JSON, will raise an error if not valid JSON\n",
    "                    return json_response\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Invalid JSON response for message {system_user_message}\\nReturning None for this request.\")\n",
    "                    return '{\"content\": \"None\"}'  # This re-raises the last exception, triggering a retry\n",
    "                except Exception as e:\n",
    "                    print(f\"Error while processing: {str(e)}\")\n",
    "                    raise\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "player2 = APIPlayer_simpler(model_name = \"gpt-35-turbo\", temperature = 1.0, max_tokens = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 14.707165002822876\n"
     ]
    }
   ],
   "source": [
    "started = time.time()\n",
    "results_2 = []\n",
    "for i in message_sequences:\n",
    "    results_2.append(player2.get_response(i))\n",
    "print(f\"Time taken: {time.time() - started}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{\"content\": \"2\"}',\n",
       " '{\"content\": \"4\"}',\n",
       " '{\"content\": \"6\"}',\n",
       " '{\"content\": \"8\"}',\n",
       " '{\"content\": \"10\"}']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_2[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As you can notice, the method processing one request at a time took approximately 14.7 seconds. While the parallel processing method significantly reduced this time to about 1.7 seconds, making it 8.6 times faster."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "debug_coopbot_env_3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
