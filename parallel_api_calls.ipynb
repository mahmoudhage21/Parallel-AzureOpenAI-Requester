{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "from openai import AzureOpenAI\n",
    "from threading import Semaphore\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from typing import List, Dict, Any\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_fixed,\n",
    "    wait_random,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Format Example\n",
    "\n",
    "The input data, `message_sequences`, should be structured as a list of message dictionaries, where each message has a `role` (either \"system\" or \"user\") and `content`. Hereâ€™s a very simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'role': 'system',\n",
       "   'content': \"You are a helpful AI assistant.  Always answer in the following json format: {'content': '2'}.\"},\n",
       "  {'role': 'user', 'content': 'What is 1 + 1?'}],\n",
       " [{'role': 'system',\n",
       "   'content': \"You are a helpful AI assistant.  Always answer in the following json format: {'content': '2'}.\"},\n",
       "  {'role': 'user', 'content': 'What is 2 + 2?'}],\n",
       " [{'role': 'system',\n",
       "   'content': \"You are a helpful AI assistant.  Always answer in the following json format: {'content': '2'}.\"},\n",
       "  {'role': 'user', 'content': 'What is 3 + 3?'}]]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message_sequences = []\n",
    "for i in range(1, 101):\n",
    "    message_sequences.append([\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.  Always answer in the following json format: {'content': '2'}.\"},\n",
    "     {\"role\": \"user\", \"content\": f\"What is {i} + {i}?\"}\n",
    "    ])\n",
    "message_sequences[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class APIRequestor:\n",
    "    def __init__(self, model_name, temperature=1.0, max_tokens=15, rate_limit=20):\n",
    "        self.client = AzureOpenAI(\n",
    "                # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning\n",
    "                api_version=\"2023-07-01-preview\",\n",
    "                # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource\n",
    "               azure_endpoint= \"https://chm-openai.openai.azure.com/\",\n",
    "            )\n",
    "        self.model_name = model_name\n",
    "        self.temperature = temperature\n",
    "        self.max_tokens = max_tokens\n",
    "        self.rate_limit = rate_limit  # max requests per minute, adjust according to your limit\n",
    "        self.semaphore = Semaphore(self.rate_limit)\n",
    "    def handle_last_retry_error(retry_state):\n",
    "        print(f\"All retry attempts failed for: {retry_state.args[0]}\\nReturning None for this request.\")\n",
    "        return '{\"content\": \"None\"}'  # Custom behavior after all retries fail\n",
    "\n",
    "    @retry(wait=wait_fixed(2) + wait_random(0, 2),\n",
    "            stop=stop_after_attempt(2),\n",
    "            before_sleep= lambda retry_state: print(\"Retrying...\"),\n",
    "            retry_error_callback=handle_last_retry_error)\n",
    "    def get_response(self, system_user_message: List):\n",
    "            with self.semaphore:\n",
    "                try:\n",
    "                    response = self.client.chat.completions.create(\n",
    "                                model=self.model_name,\n",
    "                                messages= system_user_message,\n",
    "                                temperature= self.temperature,\n",
    "                                max_tokens=self.max_tokens,\n",
    "                                response_format= { \"type\": \"json_object\" },\n",
    "                            )\n",
    "                    json_response = response.choices[0].message.content\n",
    "                    json.loads(json_response)  # Attempt to parse the JSON, will raise an error if not valid JSON\n",
    "                    time.sleep(60 / self.rate_limit)  # Pause to respect the rate limit to evenly distribute requests over time\n",
    "                    return json_response\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Invalid JSON response for message {system_user_message}\")\n",
    "                    raise  # This re-raises the last exception, triggering a retry\n",
    "                except Exception as e:\n",
    "                    print(f\"Error while processing: {str(e)}\")\n",
    "                    raise\n",
    "         \n",
    "    def get_responses_parallel(self, messages_list):\n",
    "        results = []\n",
    "        started = time.time()\n",
    "        with ThreadPoolExecutor(max_workers=self.rate_limit) as executor:\n",
    "            future_to_message = {executor.submit(self.get_response, message): message for message in messages_list}\n",
    "            for future in as_completed(future_to_message):\n",
    "                message = future_to_message[future]\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    # Extract the user's query from the message (response format is a JSON string like '{\"content\": \"4\"}')\n",
    "                    user_query = next((msg['content'] for msg in message if msg['role'] == 'user'), \"Unknown query\")\n",
    "                    result_content = json.loads(result)['content'] # if result is \n",
    "                    results.append({\"input\": user_query, \"content\": result_content})\n",
    "                except Exception as e:\n",
    "                      results.append({\"input\": \"Error\", \"content\": f\"Error processing message: {str(e)}\"})\n",
    "        print(f\"Total time taken: {time.time() - started} seconds\")\n",
    "        return results               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure you stay within the rate limit for the selected model\n",
    "\n",
    "In Our Azure AI settings, gpt-35-turbo has a Rate limit (Requests per minute) of 720"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt35_turbo_api = APIRequestor(model_name = \"gpt-35-turbo\", temperature = 1.0, max_tokens = 20, rate_limit = 100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken: 2.5722763538360596 seconds\n"
     ]
    }
   ],
   "source": [
    "results_parallel = gpt35_turbo_api.get_responses_parallel(message_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input': 'What is 53 + 53?', 'content': '106'},\n",
       " {'input': 'What is 100 + 100?', 'content': '200'},\n",
       " {'input': 'What is 42 + 42?', 'content': '84'},\n",
       " {'input': 'What is 56 + 56?', 'content': '112'},\n",
       " {'input': 'What is 19 + 19?', 'content': '38'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_parallel[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As u can notice, the order of the responses is not the same as the order of the queries, because the requests are processed in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is 53 + 53?</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is 100 + 100?</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is 42 + 42?</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is 56 + 56?</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is 19 + 19?</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>What is 38 + 38?</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>What is 45 + 45?</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>What is 65 + 65?</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>What is 48 + 48?</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>What is 49 + 49?</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 input content\n",
       "0     What is 53 + 53?     106\n",
       "1   What is 100 + 100?     200\n",
       "2     What is 42 + 42?      84\n",
       "3     What is 56 + 56?     112\n",
       "4     What is 19 + 19?      38\n",
       "..                 ...     ...\n",
       "95    What is 38 + 38?      76\n",
       "96    What is 45 + 45?      90\n",
       "97    What is 65 + 65?     130\n",
       "98    What is 48 + 48?      96\n",
       "99    What is 49 + 49?      98\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_parallel = pd.DataFrame(results_parallel)\n",
    "df_parallel.to_csv(\"results_parallel.csv\", index=False)\n",
    "df_parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now a one-per-one request method to compare it with the parallel method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class APIRequestor_simpler:\n",
    "    def __init__(self, model_name, temperature=1.0, max_tokens=15):\n",
    "        self.client = AzureOpenAI(\n",
    "                # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning\n",
    "                api_version=\"2023-07-01-preview\",\n",
    "                # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource\n",
    "               azure_endpoint= \"https://chm-openai.openai.azure.com/\",\n",
    "            )\n",
    "        self.model_name = model_name\n",
    "        self.temperature = temperature\n",
    "        self.max_tokens = max_tokens\n",
    "\n",
    "    def get_response(self, system_user_message: List):\n",
    "                try:\n",
    "                    response = self.client.chat.completions.create(\n",
    "                                model=self.model_name,\n",
    "                                messages= system_user_message,\n",
    "                                temperature= self.temperature,\n",
    "                                max_tokens=self.max_tokens,\n",
    "                                response_format= { \"type\": \"json_object\" },\n",
    "                            )\n",
    "                    json_response = response.choices[0].message.content\n",
    "                    json.loads(json_response)  # Attempt to parse the JSON, will raise an error if not valid JSON\n",
    "                    return json_response\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Invalid JSON response for message {system_user_message}\\nReturning None for this request.\")\n",
    "                    return '{\"content\": \"None\"}'  # This re-raises the last exception, triggering a retry\n",
    "                except Exception as e:\n",
    "                    print(f\"Error while processing: {str(e)}\")\n",
    "                    raise\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt35_turbo_api2 = APIRequestor_simpler(model_name = \"gpt-35-turbo\", temperature = 1.0, max_tokens = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 18.58565092086792\n"
     ]
    }
   ],
   "source": [
    "started = time.time()\n",
    "results_2 = []\n",
    "for i in message_sequences:\n",
    "    results_2.append(gpt35_turbo_api2.get_response(i))\n",
    "print(f\"Time taken: {time.time() - started}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As you can notice, processing these requests one by one took about 18.6 seconds. However, using the parallel processing method, this time was significantly reduced to approximately 2.6 seconds, making it 7 times faster.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "debug_coopbot_env_3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
